{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["! pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","ds = load_dataset('wider_face',split='train')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from PIL import Image\n","import numpy as np\n","\n","df=ds.to_pandas()\n","print(df.index)\n","print(df.faces.iloc[3]['bbox'])\n","path=df.image.iloc[3]['path']\n","def load_image(image_path):\n","    with open(image_path, 'rb') as f:\n","        img = Image.open(f)\n","        return np.array(img)\n","plt.imshow(load_image(path))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import cv2\n","path=df.image.iloc[0]['path']\n","img = cv2.imread(path)\n","img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","print(img.shape)\n","for bbox in df.faces.iloc[0]['bbox']:\n","\n","  x1, y1, x2, y2 = bbox\n","  cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n","plt.imshow(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","import csv\n","from sklearn import preprocessing\n","import cv2"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["labels=['face']\n","le = preprocessing.LabelEncoder()\n","le.fit(labels)\n","print(le.classes_)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def coco_to_yolo(coco_boxes, img_width, img_height,le):\n","    yolo_boxes = []\n","    labelcode = le.transform(['face'])[0]\n","    for box in coco_boxes:\n","        x_min, y_min, width, height = box\n","        x_center = x_min + width / 2.0\n","        y_center = y_min + height / 2.0\n","        x_center_norm = x_center / img_width\n","        y_center_norm = y_center / img_height\n","        width_norm = width / img_width\n","        height_norm = height / img_height\n","        yolo_boxes.append([labelcode,x_center_norm, y_center_norm, width_norm, height_norm])\n","    return yolo_boxes"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import shutil\n","\n","folder_path = '/kaggle/working/yolov7/split'\n","shutil.rmtree(folder_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","os.makedirs(os.path.join('images'), exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def savedatasetfhs(row):\n","  a=row.name\n","  if(a< 6500):\n","    path=row[0]['path']\n","    img=cv2.imread(path)\n","    image=tf.keras.preprocessing.image.array_to_img(load_image(row[0]['path']))\n","    image.save('/kaggle/working/images/image'+str(a)+'.jpeg')\n","    np.savetxt('/kaggle/working/images/image'+str(a)+'.txt', coco_to_yolo(row[1]['bbox'],img.shape[1],img.shape[0],le))\n","    #np.save('/content/images/image'+str(a)+'.npy', coco_to_yolo(row[1]['bbox'],img.shape[1],img.shape[0]))\n","df.apply(savedatasetfhs,axis=\"columns\")"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T10:27:36.851619Z","iopub.status.busy":"2023-10-08T10:27:36.851233Z","iopub.status.idle":"2023-10-08T10:27:55.005613Z","shell.execute_reply":"2023-10-08T10:27:55.004134Z","shell.execute_reply.started":"2023-10-08T10:27:36.851590Z"},"trusted":true},"outputs":[],"source":["%%capture\n","\n","!git clone https://github.com/WongKinYiu/yolov7 # Downloading YOLOv7 repository and installing requirements\n","%cd yolov7\n","!pip3 install -qr requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!wget \"https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt\""]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-08T10:28:24.183379Z","iopub.status.busy":"2023-10-08T10:28:24.182936Z","iopub.status.idle":"2023-10-08T10:28:27.390876Z","shell.execute_reply":"2023-10-08T10:28:27.389606Z","shell.execute_reply.started":"2023-10-08T10:28:24.183342Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["W&B disabled.\n"]}],"source":["!wandb disabled"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","from sklearn.model_selection import train_test_split\n","\n","def split_data(folder_path, train_size=0.8):\n","    # Create the output directories\n","    os.makedirs(os.path.join('split', 'images', 'train'), exist_ok=True)\n","    os.makedirs(os.path.join('split', 'images', 'val'), exist_ok=True)\n","    os.makedirs(os.path.join('split', 'labels', 'train'), exist_ok=True)\n","    os.makedirs(os.path.join('split', 'labels', 'val'), exist_ok=True)\n","\n","    # Get the list of image and label files in the folder\n","    image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpeg')]\n","    label_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n","\n","    # Sort the lists of files\n","\n","    image_files.sort()\n","    label_files.sort()\n","\n","    # Split the data into training and validation sets\n","    train_images, val_images, train_labels, val_labels = train_test_split(image_files, label_files, train_size=train_size)\n","\n","    # Copy the training images and labels to the output directories\n","    for image_file, label_file in zip(train_images, train_labels):\n","        shutil.copy(os.path.join(folder_path, image_file), os.path.join('split', 'images', 'train', image_file))\n","        shutil.copy(os.path.join(folder_path, label_file), os.path.join('split', 'labels', 'train', label_file))\n","\n","    # Copy the validation images and labels to the output directories\n","    for image_file, label_file in zip(val_images, val_labels):\n","        shutil.copy(os.path.join(folder_path, image_file), os.path.join('split', 'images', 'val', image_file))\n","        shutil.copy(os.path.join(folder_path, label_file), os.path.join('split', 'labels', 'val', label_file))\n","\n","\n","folder_path = '/kaggle/working/images'\n","split_data(folder_path)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_img_path = \"split/images/train\"\n","train_label_path = \"split/labels/train\"\n","\n","val_img_path = \"split/images/val\"\n","val_label_path = \"split/labels/val\"\n","image_files_train=[f for f in os.listdir(train_img_path) if f.endswith('.jpeg')]\n","label_files_train=[f for f in os.listdir(train_label_path) if f.endswith('.txt')]\n","image_files_val=[f for f in os.listdir(val_img_path) if f.endswith('.jpeg')]\n","label_files_val=[f for f in os.listdir(val_label_path) if f.endswith('.txt')]\n","image_files_train.sort()\n","label_files_train.sort()\n","image_files_val.sort()\n","label_files_val.sort()\n","for f in range(len(image_files_train)):\n","  image_files_train[f]=f'{image_files_train[f].split(\".\")[0]}'\n","for f in range(len(label_files_train)):\n","  label_files_train[f]=f'{label_files_train[f].split(\".\")[0]}'\n","for f in range(len(image_files_val)):\n","  image_files_val[f]=f'{image_files_val[f].split(\".\")[0]}'\n","for f in range(len(label_files_val)):\n","  label_files_val[f]=f'{label_files_val[f].split(\".\")[0]}'\n","if(len(image_files_train)==len(label_files_train) and image_files_train== label_files_train):\n","  print('sucessful train split ')\n","  print('number of train images: '+str(len(image_files_train)))\n","  print('number of train labels: '+str(len(label_files_train)))\n","else:\n","  print('Unsucessful train split')\n","if(len(image_files_val)==len(label_files_val) and image_files_val== label_files_val):\n","  print('sucessful val split ')\n","  print('number of val images: '+str(len(image_files_val)))\n","  print('number of val labels: '+str(len(label_files_val)))\n","else:\n","  print('Unsucessful val split')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# creaing .yaml file for specifying about the paths, number of classes, and class names\n","!echo -e \"train: /kaggle/working/yolov7/split/images/train\\nval: /kaggle/working/yolov7/split/images/val\\n\\nnc: 1\\nnames: ['face']\" >> rbcdet.yaml\n","!cat 'rbcdet.yaml'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["shutil.copyfile('/kaggle/working/yolov7/rbcdet.yaml', '/kaggle/working/yolov7/data/rbcdet.yaml')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!sed -i 's/iou_t: 0.2/iou_t: 0.6/'  /kaggle/working/yolov7/data/hyp.scratch.p5.yaml"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!python train.py --batch 16 --epochs 25 --data /kaggle/working/yolov7/data/rbcdet.yaml --weights '/kaggle/working/yolov7/runs/train/exp3/weights/best.pt'   --img=640 --freeze 5 --hyp data/hyp.scratch.p5.yaml"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!python detect.py --weights /kaggle/working/yolov7/runs/train/exp4/weights/best.pt --img 256 --conf 0.40 --source /kaggle/input/testdetect/"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#display inference on ALL test images\n","\n","import glob\n","from IPython.display import Image, display\n","\n","i = 0\n","limit = 10000 # max images to print\n","for imageName in glob.glob('/kaggle/working/yolov7/runs/detect/exp8/*.jpeg'):\n","    if i < limit:\n","      display(Image(filename=imageName))\n","      print(\"\\n\")\n","    i = i + 1"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'models'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myolov7\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attempt_load\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myolov7\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_img_size, non_max_suppression, scale_coords\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myolov7\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplots\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_one_box\n","File \u001b[1;32mc:\\Users\\21653\\Documents\\spcr\\mypts\\Face-Sight\\Face-Sight\\src\\models\\yolov7\\models\\experimental.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv, DWConv\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgoogle_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attempt_download\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCrossConv\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Cross Convolution Downsample\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models'"]}],"source":["import cv2\n","import torch\n","from models.experimental import attempt_load\n","from utils.general import check_img_size, non_max_suppression, scale_coords\n","from utils.plots import plot_one_box\n","from utils.torch_utils import select_device\n","# Set the device to run on\n","\n","# Load the custom YOLOv7 model\n","model = attempt_load('/kaggle/input/bestdetect1/best (1).pt')\n","names = model.module.names if hasattr(model, 'module') else model.names\n","# Set the image size\n","imgsz = check_img_size(640, s=model.stride.max())\n","\n","# Set the filename of the image\n","filename = '/kaggle/input/testdetect/image384.jpeg'\n","\n","# Read the image\n","image = cv2.imread(filename)\n","\n","# Check if the image was read correctly\n","if image is None:\n","    print('Error: Could not read image')\n","else:\n","    # Set the model's stride\n","    stride = int(model.stride.max())\n","\n","    # Compute the new size of the image\n","    new_size = (image.shape[1] - image.shape[1] % stride, image.shape[0] - image.shape[0] % stride)\n","\n","    # Resize the image\n","    image = cv2.resize(image, new_size)\n","    \n","    # Convert the image to a tensor\n","    img = torch.from_numpy(image.transpose((2, 0, 1)))\n","    \n","    # Reshape and normalize the image\n","    img = img.float()\n","    img /= 255.0\n","    if img.ndimension() == 3:\n","        img = img.unsqueeze(0)\n","    \n","    # Run the model on the image\n","    pred = model(img)[0]\n","    \n","    # Apply non-maximum suppression to the predictions\n","    pred = non_max_suppression(pred)\n","    \n","    # Process the predictions\n","    for det in pred:\n","        if len(det):\n","            # Rescale the coordinates to the original image size\n","            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], image.shape).round()\n","            \n","            # Draw the bounding boxes on the image\n","            for *xyxy, conf, cls in reversed(det):\n","                if conf > 0.4:\n","                    x1, y1, x2, y2=xyxy\n","                    label = f'{names[int(cls)]} {conf:.2f}'\n","                    face = image[int(y1):int(y2), int(x1):int(x2)]\n","                    print(face.shape)\n","                    _,face=cv2.imencode('.jpeg', face) \n","                    display(Image(face))\n","            plot_one_box(xyxy, image, label=label)\n","            _,image=cv2.imencode('.jpeg', image) \n","            display(Image(image))\n","                    \n","    "]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","c:\\Users\\21653\\Documents\\spcr\\mypts\\Face-Sight\\Face-Sight\\src\\models\n"]},{"ename":"ModuleNotFoundError","evalue":"No module named 'models'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[10], line 91\u001b[0m\n\u001b[0;32m     88\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage12.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m model, names, scale_coords, non_max_suppression, plot_one_box \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Read the image\u001b[39;00m\n\u001b[0;32m     94\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n","Cell \u001b[1;32mIn[10], line 19\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Now that the repository is cloned, we can import the utility functions.\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myolov7\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attempt_load\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myolov7\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneral\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scale_coords, non_max_suppression\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myolov7\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplots\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_one_box\n","File \u001b[1;32mc:\\Users\\21653\\Documents\\spcr\\mypts\\Face-Sight\\Face-Sight\\src\\models\\yolov7\\models\\experimental.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv, DWConv\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgoogle_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attempt_download\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCrossConv\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Cross Convolution Downsample\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'models'"]}],"source":["import cv2\n","import torch\n","import git\n","import os\n","import numpy as np\n","\n","\n","def load_model(model_path):\n","    # Check if the repository exists locally. If not, clone it.\n","    if not os.path.exists('yolov7'):\n","        try:\n","            git.Repo.clone_from('https://github.com/WongKinYiu/yolov7.git', 'yolov7')\n","        except git.GitCommandError as e:\n","            print(f\"Error occurred while cloning the repository: {e}\")\n","            return None\n","\n","    # Now that the repository is cloned, we can import the utility functions.\n","    from yolov7.models.experimental import attempt_load\n","    from yolov7.utils.general import scale_coords, non_max_suppression\n","    from yolov7.utils.plots import plot_one_box\n","\n","    model = attempt_load(model_path)\n","    names = model.module.names if hasattr(model, 'module') else model.names\n","    return model, names, scale_coords, non_max_suppression, plot_one_box\n","\n","def process_image(image, model,non_max_suppression):\n","    # Save the original image size\n","    original_size = image.shape[:2]\n","\n","    # Set the model's stride\n","    stride = int(model.stride.max())\n","\n","    # Compute the new size of the image\n","    new_size = (\n","        image.shape[1] - image.shape[1] % stride,\n","        image.shape[0] - image.shape[0] % stride,\n","    )\n","\n","    # Resize the image\n","    image = cv2.resize(image, new_size)\n","\n","    # Convert the image to a tensor\n","    img = torch.from_numpy(image.transpose((2, 0, 1)))\n","\n","    # Reshape and normalize the image\n","    img = img.float()\n","    img /= 255.0\n","    if img.ndimension() == 3:\n","        img = img.unsqueeze(0)\n","\n","    # Run the model on the image\n","    pred = model(img)[0]\n","\n","    # Apply non-maximum suppression to the predictions\n","    pred = non_max_suppression(pred)\n","\n","    return pred, image,img, original_size\n","def draw_boxes(pred, image,img,names, original_size, faces,scale_coords,plot_one_box):\n","    # Process the predictions\n","    for det in pred:\n","        if len(det):\n","            # Rescale the coordinates to the original image size\n","            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], image.shape).round()\n","\n","            # Draw the bounding boxes on the image\n","            for *xyxy, conf, cls in reversed(det):\n","                if conf > 0.4:\n","                    x1, y1, x2, y2 = xyxy\n","                    label = f\"{names[int(cls)]} {conf:.2f}\"\n","                    face = image[int(y1) : int(y2), int(x1) : int(x2)]\n","                    faces.append(face)  # Add the detected face to the list\n","                    #_, face = cv2.imencode(\".jpeg\", face)\n","                    plot_one_box(\n","                        xyxy, image, label=label, color=(255, 0, 0), line_thickness=3\n","                    )  # Draw the bounding box on the original image\n","    \n","    # Resize the image back to its original size\n","    image = cv2.resize(image, (original_size[1], original_size[0]))\n","    return (\n","        faces,\n","        image,\n","    )  # Return the list of detected faces and the original image with bounding boxes\n","# Path to your model\n","model_path = 'model.pt'\n","\n","# Path to an image file to test\n","image_path = 'image12.jpeg'\n","\n","# Load the model\n","model, names, scale_coords, non_max_suppression, plot_one_box = load_model(model_path)\n","\n","# Read the image\n","image = cv2.imread(image_path)\n","faces=[]\n","# Check if the image was read correctly\n","if image is None:\n","    print('Error: Could not read image')\n","else:\n","    # Process the image\n","    pred, image,processed_image, original_size = process_image(image, model,non_max_suppression)\n","\n","    # Draw boxes on the image\n","    faces, image_with_boxes = draw_boxes(pred, image,processed_image, names, original_size,faces,scale_coords,plot_one_box)\n","    print(image_with_boxes.shape)\n","    # Display the image with bounding boxes\n","    #_,image_with_boxes=cv2.imencode('.jpeg', image_with_boxes) \n","    #print(image_with_boxes.shape)\n","    display(Image.fromarray(np.uint8(image_with_boxes)))\n","    cv2.waitKey(0)\n","    cv2.destroyAllWindows()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":3626677,"sourceId":6304082,"sourceType":"datasetVersion"},{"datasetId":3639802,"sourceId":6324337,"sourceType":"datasetVersion"},{"datasetId":3715348,"sourceId":6437802,"sourceType":"datasetVersion"},{"datasetId":3725374,"sourceId":6452542,"sourceType":"datasetVersion"},{"datasetId":3726026,"sourceId":6453473,"sourceType":"datasetVersion"},{"datasetId":3730057,"sourceId":6459271,"sourceType":"datasetVersion"},{"datasetId":3730257,"sourceId":6459563,"sourceType":"datasetVersion"},{"datasetId":3757808,"sourceId":6500757,"sourceType":"datasetVersion"},{"datasetId":3762354,"sourceId":6507463,"sourceType":"datasetVersion"},{"datasetId":3769872,"sourceId":6520854,"sourceType":"datasetVersion"},{"datasetId":3780544,"sourceId":6539576,"sourceType":"datasetVersion"},{"datasetId":3784404,"sourceId":6547562,"sourceType":"datasetVersion"},{"datasetId":3785163,"sourceId":6549053,"sourceType":"datasetVersion"},{"datasetId":3804972,"sourceId":6593034,"sourceType":"datasetVersion"},{"datasetId":3806805,"sourceId":6597049,"sourceType":"datasetVersion"},{"datasetId":3810853,"sourceId":6605088,"sourceType":"datasetVersion"},{"datasetId":3814594,"sourceId":6610612,"sourceType":"datasetVersion"},{"datasetId":3831886,"sourceId":6637611,"sourceType":"datasetVersion"}],"dockerImageVersionId":30554,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
